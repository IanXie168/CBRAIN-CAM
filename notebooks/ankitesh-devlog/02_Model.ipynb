{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "from climate_invariant import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "                    decode_times=False)\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "scale_dict = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will be fed to the models during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For positve sepearation (required only when scaling is included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_RH = 'PosCRH_CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'PosCRH_CI_RH_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_RH_pos = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = TRAINDIR+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For negative sepearation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_RH = 'NegCRH_CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'NegCRH_CI_RH_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_RH_neg = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = TRAINDIR+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_TNS = 'CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'CI_TNS_M4K_NORM_norm.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_TNS = 'PosCRH_CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'PosCRH_CI_TNS_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_TNS_pos = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_TNS = 'NegCRH_CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'NegCRH_CI_TNS_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_TNS_neg = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we will be training two models - 1 would be brute force and the other would be the whole climate invariant network.  \n",
    "Depeneding on your requrement you can toggle the transformation flag using the below data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generator for bruteforce network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this won't be used just to show we can use it overall\n",
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'\n",
    "\n",
    "train_gen_bf = DataGeneratorClimInv(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans = False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n",
    "\n",
    "valid_gen_bf = DataGeneratorClimInv(\n",
    "    data_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans = False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For positive (for the whole network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'PosCRH_CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'PosCRH_CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'PosCRH_CI_SP_M4K_valid.nc'\n",
    "\n",
    "train_gen_pos = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_pos.input_transform.sub, inp_divRH=train_gen_RH_pos.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_pos.input_transform.sub,inp_divTNS=train_gen_TNS_pos.input_transform.div\n",
    ")\n",
    "\n",
    "valid_gen_pos = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_pos.input_transform.sub, inp_divRH=train_gen_RH_pos.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_pos.input_transform.sub,inp_divTNS=train_gen_TNS_pos.input_transform.div\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Negative (We don't scale the negative network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'NegCRH_CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'NegCRH_CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'NegCRH_CI_SP_M4K_valid.nc'\n",
    "\n",
    "### we don't scale this network\n",
    "train_gen_neg = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_neg.input_transform.sub, inp_divRH=train_gen_RH_neg.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_neg.input_transform.sub,inp_divTNS=train_gen_TNS_neg.input_transform.div,\n",
    "    scaling=False\n",
    ")\n",
    "\n",
    "valid_gen_neg = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    normalize_flag=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_neg.input_transform.sub, inp_divRH=train_gen_RH_neg.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_neg.input_transform.sub,inp_divTNS=train_gen_TNS_neg.input_transform.div,\n",
    "    scaling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(64,))\n",
    "densout = Dense(128, activation='linear')(inp)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "dense_out = Dense(64, activation='linear')(densout)\n",
    "model = tf.keras.models.Model(inp, dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "path_HDF5 = '/oasis/scratch/comet/ankitesh/temp_project/models/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+'BF.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_bf, epochs=Nep, validation_data=valid_gen_bf,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dim_size = 40 # intermediate dimension size of vertical interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(179,)) ## input after rh and tns transformation\n",
    "offset = 65\n",
    "inp_TNS = inp[:,offset:offset+2*inter_dim_size+4]\n",
    "offset = offset+2*inter_dim_size+4\n",
    "lev_tilde_before = inp[:,offset:offset+30]\n",
    "offset = offset+30\n",
    "\n",
    "densout = Dense(128, activation='linear')(inp_TNS)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "denseout = Dense(2*inter_dim_size+4, activation='linear')(densout)\n",
    "lev_original_out = reverseInterpLayer(inter_dim_size)([denseout,lev_tilde_before])\n",
    "out = ScaleOp(OpType.PWA.value,\n",
    "              inp_subQ=train_gen_pos.input_transform.sub, \n",
    "              inp_divQ=train_gen_pos.input_transform.div,\n",
    "              )([inp,lev_original_out]) #2 for root\n",
    "model = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 179)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 84)]         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 128)          10880       tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 128)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          16512       leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 128)          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128)          16512       leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 128)          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 128)          16512       leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 128)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 128)          16512       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 128)          0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          16512       leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 128)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 128)          16512       leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 128)          0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 84)           10836       leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [(None, 30)]         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reverse_interp_layer_2 (reverse (None, 64)           0           dense_31[0][0]                   \n",
      "                                                                 tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "scale_op_1 (ScaleOp)            (None, 64)           0           input_4[0][0]                    \n",
      "                                                                 reverse_interp_layer_2[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 120,788\n",
      "Trainable params: 120,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "path_HDF5 = '/oasis/scratch/comet/ankitesh/temp_project/models/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+'RH_TNS_LH_ScalePos_Interp.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 20\n",
    "model.fit_generator(train_gen_pos, epochs=Nep, validation_data=valid_gen_pos,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(178,)) ## input after rh and tns transformation\n",
    "offset = 64\n",
    "inp_TNS = inp[:,offset:offset+2*inter_dim_size+4]\n",
    "offset = offset+2*inter_dim_size+4\n",
    "lev_tilde_before = inp[:,offset:offset+30]\n",
    "offset = offset+30\n",
    "\n",
    "densout = Dense(128, activation='linear')(inp_TNS)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "denseout = Dense(2*inter_dim_size+4, activation='linear')(densout)\n",
    "out = reverseInterpLayer(inter_dim_size)([denseout,lev_tilde_before])\n",
    "model = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "path_HDF5 = '/oasis/scratch/comet/ankitesh/temp_project/models/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+'RH_TNS_LH_ScaleNeg_Interp.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen_neg, epochs=Nep, validation_data=valid_gen_neg,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CbrainCustomLayer",
   "language": "python",
   "name": "cbraincustomlayer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
